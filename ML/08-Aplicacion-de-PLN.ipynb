{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Machine Learging: Aplicacion de Procesamiento de Lenguaje Natural (PLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Importar Librerias\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Cargar y Preprocesar el Dataset IMDB\n",
    "# Número de palabras más frecuentes a considerar en el dataset\n",
    "max_features = 20000\n",
    "# Limitar las secuencias a esta longitud máxima\n",
    "maxlen = 500\n",
    "\n",
    "# Cargar el dataset IMDB\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Rellenar o truncar las secuencias para que todas tengan la misma longitud\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Construir el Modelo de la Red Neuronal (usando LSTM)\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Añadir una capa de embedding para convertir las secuencias en vectores densos de longitud fija\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Añadir una capa LSTM\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Añadir una capa densa con una única neurona de salida y activación sigmoide\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo con el optimizador Adam y la función de pérdida binary_crossentropy\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Entrenar el Modelo\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Evaluar el Modelo\n",
    "# Evaluar el modelo con los datos de prueba\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Predecir con el Modelo\n",
    "# Hacer predicciones con el modelo entrenado\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Mostrar la primera predicción\n",
    "print('Predicted sentiment:', 'positive' if predictions[0] > 0.5 else 'negative')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
